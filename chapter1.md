# Chapter 1｜范围与需求（Scope & Requirements）

## 1. 开篇：定义座舱交互的“奇点”

欢迎阅读《车舱语音实时对话机器人设计指南》。

我们正处于车载交互的转折点。过去十年，车载语音助手（Voice Assistant 1.0）主要解决的是“手忙眼乱”时的基础指令执行（如“打开空调”、“导航回家”）。然而，这些系统通常基于刚性的规则引擎和槽位填充（Slot Filling），面临着三大痛点：
1.  **交互僵硬**：必须使用特定唤醒词，无法处理打断（Barge-in），像是在发号施令而非对话。
2.  **感知割裂**：语音助手“看不见”驾驶员指着的屏幕图标，也“不知道”后排乘客正在睡觉。
3.  **生态孤岛**无法操作未开放 API 的第三方安卓应用（如外卖、订票 App）。

本项目旨在构建 **Voice Agent 2.0**。利用 **OpenAI Realtime API** 的低延迟流式多模态能力，结合 **OpenAI Agents SDK** 的任务编排能力，我们将打造一个**“眼观六路、耳听八方”的拟人化座舱副驾**。它不仅能听懂复杂的自然语言，还能看懂车内外环境，甚至像人一样操作屏幕 App。

**本章学习目标：**
1.  **界定边界**：清晰划分“智能座舱 Agent”与“自动驾驶系统”的红线。
2.  **场景拆解**：深入理解车控、RAG 问答、GUI 自动化、多模态感知的具体业务流。
3.  **约束识别**：掌握车规级环境下的特殊限制（弱网、算力、隐私、噪音）。
4.  **指标定义**：确立衡量系统成功的北极星指标（Latency, Completion Rate）。

---

## 2. 详细论述

### 2.1 价值主张与系统定位

本系统在整车电子电气架构（E/E 架构）中位于 **智能座舱域（Smart Cockpit Domain）** 的应用层与服务层之间。它是一个超级中介，向下调动车辆硬件，向上连接云端大模型与互联网生态。

#### 2.1.1 核心价值矩阵
*   **安全（Safety）**：通过语音和眼动交互减少驾驶员手离开方向盘（Hands-off）的时间。通过 DMS（驾驶员监控）主动干预疲劳驾驶。
*   **效率（Efficiency）**：一句话解决复杂任务（例如：“把座椅调舒服点，放首周杰伦的歌，顺便帮我查查胎压正不正常”）。
*   **情感（Empathy）**：利用 Realtime API 的语音到语音（S2S）能力，提供具备情绪感知和表达的陪伴体验，而非机械的 TTS 播报。

#### 2.1.2 系统上下文图 (System Context)

```ascii
      [驾驶员/乘客] (User)
            | ^
(语音/手势/视线) | | (语音/屏幕反馈/车辆动作)
            v |
+---------------------------------------------------------------+
|                **Intelligent Cockpit Voice Agent**            |
+---------------------------------------------------------------+
    |            |            |             |            |
    v            v            v             v            v
[感知输入]    [车控网关]    [GUI 运行时]    [知识库]    [云端大脑]
(Perception) (Control GW)  (GUI Runtime)  (RAG DB)   (OpenAI API)
    |            |            |             |            |
 DMS/OMS/7V   CAN/LIN总线   Android App   用户手册    Realtime Model
 麦克风阵列    空调/座椅     美团/大众     维修数据    Agents SDK
```

### 2.2 目标与非目标（Goals & Non-goals）

明确“不做”什么比“做”什么更重要，这能防止项目范围蔓延（Scope Creep）。

#### 2.2.1 核心目标 (Must-Haves)
1.  **端到端低延迟**：用户停止说话到听到声音（Voice-to-Audio Latency）在 **1秒以内**（理想目标 < 800ms）。
2.  **多模态融合**：支持“指代消解”（Deictic Resolution），即理解“**这个**是什么”、“**那里**么走”。
3.  **闭环服务**：对于不支持 API 的 App，能够通过 GUI Agent 完成从搜索到下单（支付前）的全流程。
4.  **合规与隐私**：所有敏感数据（人脸、车内影像）必须遵循“端侧脱敏”或“用户主动触发”原则。

#### 2.2.2 严格非目标 (Out of Scope)
1.  **自动驾驶决策**：Agent **绝不**直接控制油门、刹车、转向系统。即使能看懂红绿灯，也仅作提示，不作控车依据。
2.  **离线大模型推理**：虽然车机芯片算力在提升，但本项目假设核心推理依赖 **OpenAI Realtime API**（云端），端侧仅做轻量级唤醒、VAD 和降级处理。
3.  **全量视频监控**：系统不是行车记录仪，不进行 24 小时不间断的视频录制和云端存储。

### 2.3 关键业务场景深度拆解

#### 2.3.1 深度车控（Deep Vehicle Control）
*   **挑战**：传统的车控是 1:1 的指令映射。Agentic 车控需要处理**模糊意图**和**组合指令**。
*   **景示例**：
    *   *输入*：“我有点冷，而且后排那个窗户好像没关严。”
    *   *推理*：
        1.  Intent 1: 调高空调温度 OR 打开座椅加热（需结合当前温度决策）。
        2.  Intent 2: 检查后排车窗状态，若未全关则关闭。
    *   *执行*：调用 `get_window_status()` -> `close_window(id=rear_right)` -> `set_hvac(temp=26)`.
    *   *ToolCall 设计*：必须具备**参数自动补全**和**安全范围校验**能力。

#### 2.3.2 视觉增强 RAG（Visual-RAG）
*   **挑战**：用户不仅问静态知识，还结合动态视觉。
*   **场景示例**：
    *   *输入*：用户看到仪表盘亮起一个红色图标，问：“这亮的是什么灯？我该怎么办？”
    *   *流程*：
        1.  **Capture**: 获取仪表盘区域截图。
        2.  **Analyze**: 视觉模型识别图标为“发动机机油压力过低”。
        3.  **Retrieve**: RAG 检索《车主手册》中关于该故障码的章节。
        4.  **Reason**: 结合手册，生成建议（“请立即安全停车，不要继续行驶...”）。
        5.  **Respond**: 严肃、紧迫的语气播报。

#### 2.3.3 GUI Agent（车载 App 自动化）
*   **挑战**：App 界面多变，且没有标准 API。
*   **场景示例**：
    *   *输入*：“帮我在附近找一家评分 4.5 以上的日料店，要好停车的。”
    *   *流程*：
        1.  Agent 唤起“大众点评” App。
        2.  **Screen Reading**: 获取 UI 树或截图，识别“搜索框”、“筛选按钮”。
        3.  **Action**: 模拟输入“日料”，点击筛选“评分优先”、“有停车场”。
        4.  **Summary**: 滚动浏览列表，读取前三家信息，语音汇总给用户。
    *   *边界*：涉及支付密码输入时，Agent 必须暂停，提示用户“请手动完成支付”。

#### 2.3.4 驾驶员监控与主动关怀（Proactive DMS）
*   **挑战**：如何平衡“打扰”与“安全”。
*   **场景示例**：
    *   *触发*：DMS 检测到驾驶员 `distraction_level > 0.8` (视线离开路面) 持续 3秒。
    *   *流程*：
        1.  系统静音当前的娱乐音频。
        2.  Realtime API 接收 `event: distraction_alert`。
        3.  Agent 以**高优先级**插入对话：“前方路况复杂，请集中注意力。”
        4.  若未缓解，自动调低车内温度或收紧安全带（触觉反馈）。

#### 2.3.5 多模态感知输入（7V + OMS）
*   **场景示例**：
    *   *OMS (后排)*：检测到婴儿在哭。Agent 主动建议：“宝宝好像不舒服，需要我播放白噪音或调暗后排灯光吗？”
    *   *7V (车外)*：用户问“右边那个红色的店是干嘛的？”。系统调用右侧摄像头，结合 GPS 和 OCR 识别店招，回答用户。

### 2.4 外部约束与假设（Constraints）

#### 2.4.1 网络环境 (The "Tunnel" Problem)
*   **约束**：车辆是移动的，网络连接（4G/5G）不稳定。
*   **假设**：Realtime API 需要稳定的 WebSocket 连接。
*   **对策**：
    *   **心跳机制**：毫秒级监测连接质量。
    *   **混合架构**：具备本地兜底（Fallback）指令集。当断网时，切换到本地轻量级 ASR+NLU，仅支持基础车控（如“打开空调”），并语音告知“当前网络不佳，仅支持基础指令”。

#### 2.4.2 声学环境 (The "Cocktail Party")
*   **约束**：车内有胎噪、风噪、音乐声、多人说话。
*   **要求**：必须前置 **AEC（回声消除）**、**NS（噪声抑制）** 和 **BSS（盲源分离）**。
*   **Realtime 适配**：必须向 Realtime API 发送经过降噪处理的纯净人声，否则模型会把路噪当成语音尝试去理解。

#### 2.4.3 硬件算力
*   **约束**：虽然 8295 芯片算力强大，但要分给 3D 渲染、游戏等。留给 Voice Agent 的 NPU/CPU 资源有限。
*   **Rule of Thumb**：图像上传前必须在端侧压缩。例如，将 4K 摄像头画面 Downsample 到 512x512 或 1024x1024，并为 JPEG，以减少带宽和 Token 消耗。

#### 2.4.4 合规与法律
*   **中国市场**：根据《汽车数据安全管理若干规定》，车内视频数据原则上不默认出车。
*   **对策**：默认只在本地做推理（如 DMS 事件），只有在用户明确触发问答（User Initiated）时，才针对该次请求上传瞬间截图，且需并在屏幕上有明显提示（如录音/录像指示灯）。

### 2.5 需求列表 (Requirements Traceability)

| ID | 类别 | 需求名称 | 详细描述 | 优先级 |
| :--- | :--- | :--- | :--- | :--- |
| **FR-01** | **交互** | **全双工流式对话** | 支持随时打断（Barge-in），支持语气克隆或情感化 TTS。 | P0 |
| **FR-02** | **车控** | **原子化工具链** | 封装 50+ 个原子车控接口（Set/Get），支持 ToolCall 调用。 | P0 |
| **FR-03** | **感知** | **视觉上下文注入** | 支持将 DMS/OMS/7V/屏幕截图作为对话上下文（Context）输入。 | P1 |
| **FR-04** | **RAG** | **可信手册问答** | 基于向量数据库检索车主手册，回答必须附带页码或引用链接。 | P1 |
| **FR-05** | **GUI** | **泛化 App 控制** | 基于 Accessibility 或 Vision 的 App 操控，支持至少 5 个主流 App。 | P2 |
| **FR-06** | **安全** | **驾驶模式门禁** | 当车速 > X km/h 时，自动禁用视频播放、复杂 GUI 操作等高风险功能。 | P0 |
| **NFR-01** | **性能** | **首字延迟** | VAD 结束到 TTS 首个音频包播放 < 1000ms (P90)。 | P0 |
| **NFR-02** | **隐私** | **数据最小化** | 图像数据非必要不上云，上云必脱敏。 | P0 |

### 2.6 风险清单

1.  **幻觉风险 (Hallucination)**：模型可能会自信地编造一个不存在的车辆功能（例如“已启动飞行模式”）。
    *   *缓解*：严格的 System Prompt 约束 + RAG 检索校验 + 车控指令的有效性校验（Validator）。
2.  **误触发风险**：乘客聊天提到“我想吃火锅”，系统误以为是指令而打断。
    *   *缓解*：更精准的唤醒策略（视线检测 + 关键词），以及 Realtime API 的 VAD 灵敏度动态调整。
3.  **GUI 易碎性**：第三方 App 更新导致 UI 结构变化，Agent 无法定位元素。
    *   *缓解*：使用基于视觉语义（Vision-based）的定位而非硬编码坐标；建立云端热更新机制。

---

## 3. 本章小结

本章明确了我们正在构建的是一个**车规级、多模态、实时交互的智能体系统**。

*   **What**: 一个集成了 OpenAI Realtime API (耳/口) 和 Agents SDK (脑) 的座舱副驾。
*   **Why**: 解决传统语音助手交互僵硬、感知能力弱、服务生态割裂的问题。
*   **Constraints**: 必须在弱网、高噪、低算力预算和严格隐私法规的夹缝中求生存。

**Rule of Thumb (经验法则)：**
> 如果一个功能涉及到底盘控制（如加速），**永远不要**让 AI Agent 直接接管；
> 如果一个功能涉及到即时反馈（如音量调节），**一定要**提供本地离线路径作为备份。

---

## 4. 练习题

### 基础题

<details>
<summary><strong>Q1: 为什么车载场景下，GUI Agent 比传统的 API 对接更具扩展性，但稳定性更差？</strong> (Hint: 封闭生态 vs. 视觉识别)</summary>

*   **答案**：
    *   **扩展性**：API 对接需要与每个 App 厂商（美团、爱奇艺等）进行商务谈判和定制开发，周期长、成本高。GUI Agent 模拟人类操作，理论上可以操作任何已安装的 App，无需厂商配合。
    *   **稳定性**：API 是契约，通常向后兼容且稳定。GUI Agent 依赖界面元素（UI Tree/截图），一旦 App 版本更新改变了布局、图标或文案，Agent 就可能找不到目标，导致任务失败。
</details>

<details>
<summary><strong>Q2: 在“多模态输入”场景中，为什么要区分“事件触发（Event-triggered）”和“持续流式（Continuous Streaming）”上传？</strong> (Hint: 带宽与隐私)</summary>

*   **答案**：
    *   **持续流式**：摄像头画面像直播一样不间断传给大模型。这在车端不仅消耗巨大的 4G/5G 流量（成本高），还会迅速耗尽车机与云端的带宽，增加延迟，且存在极大的隐私监控风险。
    *   **事件触发**：仅在 VAD 检测到用户说话、或 DMS 检测到异常时，截取关键帧上传。这符合“数据最小化”原则，降低了成本和隐私风险。本项目采用此方案。
</details>

<details>
<summary><strong>Q3: 解释什么是“指代消解（Deictic Resolution）”在车载场景的应用。</strong> (Hint: 这个、那个、这里)</summary>

*   **答案**：
    *   指代消解是指系统理解代词（如“这个”、“那个”、“它”）具体指代对象的能力。
    *   在车载场景中，多模态输入是关键。例如用户说“**这个**怎么关掉？”，系统必须结合**视线追踪（Gaze Tracking）**（看的是中控屏还是后视镜？）或**手势识别**（指着哪个图标？），才能知道这个”指的是“车道保持辅助开关”。
</details>

### 挑战题

<details>
<summary><strong>Q4: 架构设计思考：如果 Realtime API 的平均延迟是 800ms，但在隧道中丢包率达到 30%，你应该如何设计系统架构以保证用户不会觉得系统“死机”了？</strong> (Hint: 状态机、本地反馈、非阻塞)</summary>

*   **答案**：
    1.  **即时反馈（Acknowledgment）**：在 VAD 检测到用户说完话的瞬间（<100ms），本地立即播放一个“思考音效”或让虚拟形象做出“倾听/思考”的动画。这买到了 1-2 秒的心理等待时间。
    2.  **混合路由（Hybrid Routing）**：
        *   端侧 NLU 并行分析。如果意图是简单的“打开车窗”，且云端在 500ms 内无响应，直接执行本地指令。
    3.  **流式重试与平滑**：WebRTC 本身有丢包重传（NACK/FEC），但在应用层，如果 3秒无音频流返回，应触发降级逻辑，TTS 播报“网络信号弱，请稍后再”，而不是无限静默。
    4.  **推测执行（Speculative Execution）**：(高级) 对于概率极高的后续动作，可以预加载数据。
</details>

<details>
<summary><strong>Q5: 安全与伦理：如果用户在驾驶过程中情绪激动，命令 Agent “把所有车窗打开，我要透透气”，而此时车速为 120km/h，Agent 应该如何处理？</strong> (Hint: 意图识别 vs. 安全规则)</summary>

*   **答案**：
    1.  **意图识别**：正确识别用户意图是 `open_all_windows()`。
    2.  **安全规则校验（Guardrails）**：Agent 在执行 ToolCall 之前，必须查询 `vehicle_speed`。
    3.  **规则冲突**：发现车速 120km/h，全开车窗会产生极大的风阻、噪音，甚至影响车辆稳定性（安全风险）。
    4.  **拒绝与协商**：
        *   **拒绝**：不执行全开指令。
        *   **解释**：TTS 回复“当前车速过快，全开车窗不安全。”
        *   **替代方案**：提出建议“我可以您把天窗翘起，或者打开内循环通风，可以吗？”
</details>

---

## 5. 常见陷阱与错误 (Gotchas)

### 5.1 “全能神”谬误 (The God Mode Fallacy)
*   **错误想法**：认为接了大模型，车上所有几百个功能都能控制，无需一一开发工具。
*   **现实**：大模型只能输出文本意图。要真正控制车，必须由工程师编写对应的 **Function/Tool 定义**，并将意图映射到底层 CAN 信号。如果你没写“控制后雾灯”的 Tool，模型说得再天花乱坠也控制不了。
*   **调试技巧**：从高频 Top 50 功能开始，逐步扩充 Tool 库，而不是一次性全量接入。

### 5.2 忽视 VAD 的截断问题
*   **错误现象**：用户说“打开空...调，还有...”，因为中间停顿，Realtime API 判定说话结束，立即打断用户并执行了“打开空调”。
*   **后果**：用户体验极差，感觉被抢话。
*   **调试技巧**：
    *   在云端配置更宽松的 `turn_detection` 参数（如 `silence_duration_ms`）。
    *   在端侧引入**多模态 VAD**：如果摄像头看到用户嘴巴还在动，或者视线还盯着屏幕，即使有静音也不发送“说话结束”信号。

### 5.3 隐私合规的“事后诸葛亮”
*   **错误做法**：先做完功能，全量上传图片，上线前才发现违反了欧盟 GDPR 或中国《汽车数据安全管理规定》。
*   **后果**：重构架构，甚至面临巨额罚款。
*   **Rule of Thumb**：架构设计第一天就必须设计**“隐私开关”**和**“本地脱敏层”**。任何上传云端的图像链路，中间都必须经过一个负责裁剪、模糊化、去标识的 Filter 模块。
